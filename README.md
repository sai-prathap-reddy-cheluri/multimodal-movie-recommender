# üé¨ Multimodal Movie Recommender ‚Äì Data Foundation

A fast, modern data pipeline for building a next‚Äëgen movie recommender.
It uses async parallel fetching to gather TMDb movies for a time range, then backfills runtime, actors (full cast), and directors via credits. From there, we convert to typed Parquet, publish a small sample for reviewers, and keep the full dataset as a release asset. This repo is tuned for portfolio-readability and reproducibility.

Attribution: This product uses the TMDb API but is not endorsed or certified by TMDb.

## ‚ú® Features
- **Recursive windowing** to bypass TMDb‚Äôs 10k results/query cap
- **Async credits fetch** (runtime, full cast, directors) with controlled concurrency
- **Robust backfill** script: retries, exponential backoff, batch progress writes
- Clean, reproducible **Gradio UI** to download datasets quickly
- Typed Parquet + partitioning (fast loading, analytics‚Äëfriendly)

## üóÇÔ∏è Project Layout

```
‚îú‚îÄ data/                    # Raw CSVs you generate (git‚Äëignored; keep only small samples)
‚îÇ  ‚îî‚îÄ processed/
‚îÇ     ‚îú‚îÄ movies.parquet               # full typed dataset (Release asset, not in Git)
‚îÇ     ‚îú‚îÄ movies_parquet/              # partitioned by year/ (Release asset)
‚îÇ     ‚îú‚îÄ movies_sample.parquet        # small sample kept in Git
‚îÇ     ‚îî‚îÄ splits/                      # optional time-based splits
‚îú‚îÄ reports/
‚îÇ  ‚îú‚îÄ data_profile.json               # row counts, nulls, dtypes
‚îÇ  ‚îî‚îÄ checksums.txt                   # sha256 for integrity verification
‚îú‚îÄ notebooks/
‚îÇ  ‚îî‚îÄ 01_eda_movies.ipynb             # visual EDA with short commentary
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ config.py                       # reads .env, defines DATA_DIR, etc.
‚îÇ  ‚îú‚îÄ download_dataset.py             # Gradio app (UI) to download datasets
‚îÇ  ‚îú‚îÄ tmdb_api_test.py                # quick API smoke test
‚îÇ  ‚îú‚îÄ data/
‚îÇ  ‚îÇ  ‚îî‚îÄ data_preparation.py        # intake ‚Üí Parquet ‚Üí sample ‚Üí reports
‚îÇ  ‚îú‚îÄ scripts/
‚îÇ  ‚îÇ  ‚îî‚îÄ backfill_credits.py          # CLI backfill for blank actors/directors
‚îú‚îÄ requirements.txt
‚îî‚îÄ README.md
```

## üîë Prerequisites
- Python 3.10+
- A free TMDb API key: https://www.themoviedb.org/settings/api

Prerequisite: .env in the project root with:
```ini
TMDB_API_KEY=YOUR_TMDB_KEY_HERE
```
## üß∞ Setup
```bash
# create & activate venv (Windows PowerShell)
python -m venv venv
.\venv\Scripts\activate

# macOS / Linux
python -m venv venv
source venv/bin/activate

# install deps
pip install -r requirements.txt
```

## ‚úÖ Verify your TMDb API setup

You can quickly check that your API key and network access are working by running the test script **`src/tmdb_api_test.py`**. It will call TMDb and print the top 5 popular movies.

> Run these commands **from the project root** (where your `.env` lives).

**Windows / PyCharm terminal**
```powershell
python -m src.tmdb_api_test
# or
python src\tmdb_api_test.py
```

```bash
python -m src.tmdb_api_test
# or
python src/tmdb_api_test.py
```

## üöÄ Run the Downloader (Gradio UI)

Launch the UI and download a dataset for any date range; it will also enrich with runtime, full cast, and directors.
```bash
# from repo root
python -m src.download_dataset
```
* Choose a preset date range or set custom dates.
* Toggle Include adult (18+) as desired.
* Set Concurrency (parallel credit requests). Start with 12‚Äì20.
* Output CSV: data/movies_YYYY-MM-DD_YYYY-MM-DD.csv.

## üîÅ Backfill Missing Credits (CLI)

While fetching the actors and director if API times out and the actors and director list is blank,  then can backfill rows where both actors and directors are empty, run:

```bash
# Windows (PyCharm terminal uses the project interpreter)
python -m src.scripts.backfill_credits --csv data/movies_2020-01-01_2025-08-08.csv --concurrency 12 --batch-size 800

# macOS / Linux
python -m src.scripts.backfill_credits --csv data/movies_2020-01-01_2025-08-08.csv --concurrency 12 --batch-size 800
```

What it does
* Reads your CSV.
* Finds rows where both actors and directors are blank.
* Fetches credits with retries + exponential backoff + jitter.
* Writes progress back to the same CSV after each batch so you can safely stop/resume.

## ‚öôÔ∏è Concurrency & Batch Size

* Concurrency (credits): start 12‚Äì16, cap around 20‚Äì24.
* Batch size: 500‚Äì1000 IDs per batch is a sweet spot.
* If you see 429 Too Many Requests or timeouts, lower concurrency and/or batch size.

### üß™ Quick Smoke Test
```bash
# tiny range to validate everything
python -m src.scripts.backfill_credits --csv data/movies_2025-01-01_2025-01-31.csv --concurrency 8 --batch-size 300
```

## üß± Step 1 ‚Äî Intake & Validation (convert CSV ‚Üí Parquet)

Turn the raw CSV into a typed, analytics‚Äëready Parquet dataset, plus a 10k sample and integrity reports. This step also builds absolute poster/backdrop URLs and standardizes list fields to JSON strings.

```
# requires: pandas, pyarrow
python src/data/prepare_ds_release.py data/movies_2020-01-01_2025-08-08.csv
```

### Outputs:
* data/processed/movies.parquet
* data/processed/movies_parquet/ (partitioned by year)
* data/processed/movies_sample.parquet
* reports/data_profile.json, reports/checksums.txt

### Viewing Parquet
```
import pandas as pd, duckdb
print(pd.read_parquet('data/processed/movies.parquet').head())
con = duckdb.connect()
print(con.execute("SELECT year, COUNT(*) FROM 'data/processed/movies_parquet' GROUP BY year ORDER BY year").df())
```

## üìä Step 2 ‚Äî Exploratory Data Analysis (EDA)

A compact, portfolio-ready EDA to understand coverage, data quality, and biases before modeling.
### What you‚Äôll see
- Year trend üóìÔ∏è (coverage & recency), Runtime ‚è±Ô∏è (typical lengths & outliers)
- Language & Genre mix üåç
- Popularity skew (votes vs. popularity) üìà
- Missingness heatmap üßº
- A mini poster gallery for quick eyeballing üéûÔ∏è

### Run it

Open `notebooks/01_eda_movies.ipynb`.

### ‚úÖ What this EDA tells us
- We have strong recent coverage ‚Üí use time-based splits.
- Runtimes have outliers ‚Üí clip to p95 when featurizing.
- Popularity is skewed ‚Üí add semantic retrieval to reduce popularity bias.
- Missingness is localized ‚Üí impute/skip per-feature, don‚Äôt blanket-drop rows.

## Step 3 ‚Äî Baseline ‚Äúnext-gen‚Äù recommender (embeddings + light rerank)

Build a fast, modern baseline that looks great in a portfolio: dense retrieval over rich movie text, then a tiny reranker for relevance + optional diversity.

### What you‚Äôll build
- **Text retriever (dense):** create a rich doc per movie (title ¬∑ overview ¬∑ top cast/crew ¬∑ genres ¬∑ year), embed with a compact model, and index with FAISS.
- **Light reranker:** sort the top candidates with either a **blend** (retrieval score + recency + popularity) or a **small cross-encoder**.
- **Diversity (optional):** **MMR** (Maximal Marginal Relevance) to avoid near-duplicates.
- **Tiny eval:** quick proxy metrics to claim improvements (genre overlap@k, year gap, language match).
- **Mini demo:** Streamlit one-box search with posters.

> **Trend note (verified best practice):** Hybrid **dense+sparse** retrieval plus a **light reranker** is the current default pattern for modern recommenders/search. Add MMR for diversity and you‚Äôve got a strong baseline.

---

### Artifacts produced
- `data/processed/artifacts/text.index` ‚Äî FAISS index (cosine/IP).
- `data/processed/artifacts/text_idmap.parquet` ‚Äî rowid ‚Üî movie_id map.
- `data/processed/artifacts/search_payload.parquet` ‚Äî lean fields for display/rerank.
- `data/processed/artifacts/eval_proxy_[method].csv|json` ‚Äî quick offline metrics.

---

### Run it (Step 3A‚Äì3D)

> **Windows 11 + CUDA (verified):** GPU is used for embeddings/rerank; keep FAISS on CPU.

**3A ‚Äî Build the text index**
```bash
python -m src.recsys.build_text_index
```

**3B ‚Äî Search + rerank
```bash
# Blend (default): retrieval + recency + popularity
python -m src.recsys.search_and_rerank "smart heist thriller set in Europe" --k 20 --method blend

# Retrieval only
python -m src.recsys.search_and_rerank "lonely space survival drama" --k 20 --method retrieval

# Cross-encoder rerank (small, runs on GPU)
python -m src.recsys.search_and_rerank "neo-noir crime with witty dialogue" --k 20 --method ce

# MMR diversity (more variety; Œª‚âà0.2‚Äì0.3 is a good default)
python -m src.recsys.search_and_rerank "cozy holiday romcom" --k 20 --method mmr --mmr_lambda 0.3
```

**3C ‚Äî Tiny proxy eval
```bash
python -m src.recsys.eval_proxy --k 10 --sample_n 200 --method blend
```

**3D ‚Äî Streamlit demo
```bash
streamlit run src/app/demo.py
```
### Configuration

- Models (env-overridable):
    * EMBED_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
    * CROSS_ENCODER_NAME=cross-encoder/ms-marco-MiniLM-L-6-v2

- Performance toggles:
    * ST_FP16=1 (use half precision on CUDA-capable GPUs)
    * HF_HOME=.hf_cache (shorter Windows paths)

- Hugging Face auth (avoid 429s):
    * .env: HUGGING_FACE_HUB_TOKEN=hf_xxx
    * (Optional) pre-download model and set EMBED_MODEL_NAME to the local folder

### How to search (good queries)

Use natural language: [genre] + [vibe] + [hook/theme] + [setting/locale] + [constraints]
- ‚Äúslow-burn sci-fi about isolation in space‚Äù
- ‚ÄúIndian Malayalam investigative thriller after 2020‚Äù
- ‚Äúlike ‚ÄòDrishyam‚Äô, tight family crime with twists‚Äù
- ‚Äúanime coming-of-age with music and friendship‚Äù

If results feel same-y ‚Üí use --method mmr --mmr_lambda 0.25.
If results feel too niche ‚Üí raise min vote_count in the demo or add broader vibe words.

## üß≠ Roadmap

* Poster caching on-demand
* Embeddings & vector store for RAG-augmented recommendations
* Multimodal ranking with poster/text features

## üìú License

MIT

---
